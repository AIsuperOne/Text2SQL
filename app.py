import streamlit as st
import pandas as pd
from modules.rag_engine import RAGEngine
from modules.training_manager import BatchTrainer
from utils.visualize import auto_visualize

st.set_page_config(page_title="NL2SQL RAG Demo", layout="wide")

# åˆå§‹åŒ–
if "rag_engine" not in st.session_state:
    st.session_state.rag_engine = RAGEngine()
if "trainer" not in st.session_state:
    st.session_state.trainer = BatchTrainer()

rag_engine = st.session_state.rag_engine
trainer = st.session_state.trainer

# åˆ›å»ºæ ‡ç­¾é¡µ
tabs = st.tabs(["ğŸ§‘â€ğŸ’» è‡ªç„¶è¯­è¨€æŸ¥è¯¢", "âš™ï¸ æ‰¹é‡/å¢é‡è®­ç»ƒ"])

# 1. æŸ¥è¯¢ç•Œé¢
with tabs[0]:
    st.header("è‡ªç„¶è¯­è¨€è½¬SQLæŸ¥è¯¢")
    
    # æ·»åŠ è°ƒè¯•æ¨¡å¼å¼€å…³
    debug_mode = st.checkbox("æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯", value=False)
    
    # ä½¿ç”¨ text_input è€Œä¸æ˜¯ text_area
    question = st.text_input("è¯·è¾“å…¥æ‚¨çš„ä¸šåŠ¡é—®é¢˜", placeholder="å¦‚ï¼šæŸ¥è¯¢é”€é‡æœ€é«˜çš„å‰10ä¸ªå•†å“")
    
    if st.button("æ‰§è¡ŒæŸ¥è¯¢", type="primary"):
        if not question.strip():
            st.warning("è¯·è¾“å…¥é—®é¢˜ï¼")
        else:
            with st.spinner("æ­£åœ¨æ¨ç†..."):
                try:
                    # å¦‚æœå¼€å¯è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºä¸­é—´æ­¥éª¤
                    if debug_mode:
                        with st.expander("è°ƒè¯•ä¿¡æ¯", expanded=True):
                            st.write("1. å‘é‡åŒ–é—®é¢˜...")
                            q_embed = rag_engine.embedder.embed(question)
                            st.write(f"å‘é‡ç»´åº¦: {len(q_embed)}")
                            
                            st.write("2. æ£€ç´¢æ–‡æ¡£...")
                            docs = rag_engine.vector_db.search(q_embed, top_k=5)
                            st.write(f"æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
                            for i, doc in enumerate(docs[:3]):
                                st.write(f"æ–‡æ¡£{i+1}: {doc[:100]}...")
                    
                    result = rag_engine.ask(question)
                    
                    # æ˜¾ç¤ºç”Ÿæˆçš„SQL
                    st.subheader("ç”Ÿæˆçš„SQLæŸ¥è¯¢")
                    st.code(result["sql"], language="sql")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                    if "error" in result:
                        st.error(f"æ‰§è¡Œé”™è¯¯: {result['error']}")
                    
                    # æ˜¾ç¤ºæŸ¥è¯¢ç»“æœ
                    if result["result"] is not None and not result["result"].empty:
                        st.subheader("æŸ¥è¯¢ç»“æœ")
                        st.dataframe(result["result"])
                        
                        # è‡ªåŠ¨æ™ºèƒ½å¯è§†åŒ–
                        st.subheader("æ•°æ®å¯è§†åŒ–")
                        auto_visualize(result["result"])
                    elif "error" not in result:
                        st.info("æŸ¥è¯¢æ— ç»“æœ")
                        
                except Exception as e:
                    st.error(f"ç³»ç»Ÿé”™è¯¯ï¼š{str(e)}")
                    if debug_mode:
                        st.exception(e)

# 2. æ‰¹é‡/å¢é‡è®­ç»ƒç•Œé¢
with tabs[1]:
    st.header("æ‰¹é‡/å¢é‡è®­ç»ƒæ•°æ®ä¸Šä¼ ")
    
    # åˆå§‹åŒ–å˜é‡ - è¿™æ˜¯å…³é”®ä¿®æ”¹ï¼
    ddl_list = []
    doc_list = []
    qa_pairs = []
    
    # æ–°å¢ï¼šè‡ªåŠ¨å¯¼å…¥æ•°æ®åº“ç»“æ„
    st.subheader("0. è‡ªåŠ¨å¯¼å…¥æ•°æ®åº“ç»“æ„")
    col1, col2 = st.columns([3, 1])
    with col1:
        st.info("ä»å·²è¿æ¥çš„MySQLæ•°æ®åº“è‡ªåŠ¨å¯¼å…¥è¡¨ç»“æ„ä¿¡æ¯")
    with col2:
        if st.button("ğŸ”„ å¯¼å…¥æ•°æ®åº“ç»“æ„", type="secondary"):
            with st.spinner("æ­£åœ¨è¯»å–æ•°æ®åº“ç»“æ„..."):
                try:
                    # è·å–æ•°æ®åº“ç»“æ„
                    schema_info = rag_engine.db.get_schema_info()
                    
                    # å‡†å¤‡è®­ç»ƒæ•°æ®
                    auto_ddl_list = []
                    auto_doc_list = []
                    
                    for table_info in schema_info:
                        # æ·»åŠ DDL
                        auto_ddl_list.append(table_info['ddl'])
                        
                        # æ·»åŠ è¡¨è¯´æ˜æ–‡æ¡£
                        table_doc = f"è¡¨å: {table_info['table_name']}"
                        if table_info['table_comment']:
                            table_doc += f", è¯´æ˜: {table_info['table_comment']}"
                        
                        # æ·»åŠ åˆ—è¯´æ˜
                        col_docs = []
                        for col in table_info['columns']:
                            if col.get('COLUMN_COMMENT'):  # ä½¿ç”¨ get æ–¹æ³•é¿å… KeyError
                                col_docs.append(f"{col['COLUMN_NAME']}: {col['COLUMN_COMMENT']}")
                        
                        if col_docs:
                            table_doc += ", å­—æ®µè¯´æ˜: " + "; ".join(col_docs)
                        
                        auto_doc_list.append(table_doc)
                        
                        # è·å–æ ·ä¾‹æ•°æ®
                        try:
                            sample_df = rag_engine.db.get_sample_data(table_info['table_name'], limit=3)
                            if not sample_df.empty:
                                sample_doc = f"è¡¨{table_info['table_name']}çš„æ ·ä¾‹æ•°æ®: {sample_df.to_string()}"
                                auto_doc_list.append(sample_doc)
                        except:
                            pass  # å¿½ç•¥è·å–æ ·ä¾‹æ•°æ®çš„é”™è¯¯
                    
                    # æ‰§è¡Œè®­ç»ƒ
                    counts = trainer.train_incremental(auto_ddl_list, auto_doc_list, [])
                    
                    st.success(f"âœ… æˆåŠŸå¯¼å…¥ {len(schema_info)} ä¸ªè¡¨çš„ç»“æ„ä¿¡æ¯ï¼")
                    
                    # æ˜¾ç¤ºå¯¼å…¥çš„è¡¨
                    with st.expander("æŸ¥çœ‹å¯¼å…¥çš„è¡¨"):
                        for table_info in schema_info:
                            st.write(f"- {table_info['table_name']}: {table_info.get('table_comment', '')}")
                    
                except Exception as e:
                    st.error(f"å¯¼å…¥å¤±è´¥: {str(e)}")
    
    st.divider()

    # DDLè¾“å…¥
    st.subheader("1. å¯¼å…¥ DDL è¯­å¥")
    ddl_input = st.text_area("æ¯è¡Œä¸€ä¸ªDDLå»ºè¡¨è¯­å¥", height=150, key="ddl_input")
    if ddl_input:  # åªæœ‰åœ¨æœ‰è¾“å…¥æ—¶æ‰å¤„ç†
        ddl_list = [x.strip() for x in ddl_input.split('\n') if x.strip()]
        if ddl_list:
            st.info(f"å·²è¾“å…¥ {len(ddl_list)} æ¡DDLè¯­å¥")

    # æ–‡æ¡£è¾“å…¥
    st.subheader("2. å¯¼å…¥ä¸šåŠ¡æ–‡æ¡£")
    doc_input = st.text_area("æ¯è¡Œä¸€æ®µä¸šåŠ¡æ–‡æ¡£å†…å®¹", height=150, key="doc_input")
    if doc_input:  # åªæœ‰åœ¨æœ‰è¾“å…¥æ—¶æ‰å¤„ç†
        doc_list = [x.strip() for x in doc_input.split('\n') if x.strip()]
        if doc_list:
            st.info(f"å·²è¾“å…¥ {len(doc_list)} æ¡æ–‡æ¡£")

    # é—®ç­”å¯¹æ–‡ä»¶ä¸Šä¼ 
    st.subheader("3. å¯¼å…¥SQLé—®ç­”å¯¹")
    
    # æä¾›ç¤ºä¾‹æ ¼å¼
    with st.expander("æŸ¥çœ‹æ–‡ä»¶æ ¼å¼è¦æ±‚"):
        st.write("CSVæˆ–Excelæ–‡ä»¶éœ€è¦åŒ…å«ä»¥ä¸‹ä¸¤åˆ—ï¼š")
        example_df = pd.DataFrame({
            "question": ["æ¹–åŒ—5Gç½‘ç»œçš„5GåŸºç«™å’Œ5Gå°åŒºæ•°é‡", "æŸ¥è¯¢å„åœ°å¸‚çš„5GåŸºç«™æ•°"],
            "sql": [
                "SELECT `çœä»½`,COUNT(DISTINCT station_name) AS `5gåŸºç«™æ•°`, COUNT(DISTINCT cell_name) AS `5gå°åŒºæ•°` FROM btsbase WHERE `çœä»½` = 'æ¹–åŒ—çœ' GROUP BY `çœä»½`;",
                "SELECT `åœ°å¸‚`, COUNT(DISTINCT station_name) AS `5gåŸºç«™æ•°` FROM btsbase GROUP BY `åœ°å¸‚`;"
            ]
        })
        st.dataframe(example_df)
    
    qa_file = st.file_uploader("ä¸Šä¼ é—®ç­”å¯¹æ–‡ä»¶ï¼ˆcsv/xlsxï¼‰", type=["csv", "xlsx"])
    
    if qa_file:
        try:
            if qa_file.name.endswith(".csv"):
                df_qa = pd.read_csv(qa_file)
            else:
                df_qa = pd.read_excel(qa_file)
                
            if "question" not in df_qa.columns or "sql" not in df_qa.columns:
                st.error("âŒ æ–‡ä»¶å¿…é¡»åŒ…å« 'question' å’Œ 'sql' ä¸¤åˆ—")
                qa_pairs = []  # ç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿæœ‰å€¼
            else:
                qa_pairs = df_qa[["question", "sql"]].dropna().to_dict("records")
                st.success(f"âœ… å·²è¯»å– {len(qa_pairs)} æ¡é—®ç­”å¯¹")
                
                # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®é¢„è§ˆ
                if st.checkbox("é¢„è§ˆæ•°æ®"):
                    st.dataframe(df_qa.head())
        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")
            qa_pairs = []  # ç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿæœ‰å€¼

    # è®­ç»ƒé€‰é¡¹
    st.divider()
    incremental = st.checkbox("ä»…å¢é‡è®­ç»ƒï¼ˆè·³è¿‡å·²å…¥åº“å†…å®¹ï¼‰", value=True)
    
    # æ˜¾ç¤ºå¾…è®­ç»ƒæ•°æ®ç»Ÿè®¡
    total_items = len(ddl_list) + len(doc_list) + len(qa_pairs)
    if total_items > 0:
        st.info(f"ğŸ“Š å¾…è®­ç»ƒæ•°æ®ç»Ÿè®¡ï¼šDDL {len(ddl_list)}æ¡ï¼Œæ–‡æ¡£ {len(doc_list)}æ¡ï¼Œé—®ç­”å¯¹ {len(qa_pairs)}æ¡")
    
    # å¼€å§‹è®­ç»ƒæŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary", disabled=(total_items == 0)):
        with st.spinner("è®­ç»ƒä¸­ï¼Œè¯·ç¨å€™..."):
            try:
                if incremental:
                    counts = trainer.train_incremental(ddl_list, doc_list, qa_pairs)
                    st.success(f"âœ… å¢é‡è®­ç»ƒå®Œæˆï¼æ–°å¢ï¼šDDL {counts['ddl']}æ¡ï¼Œæ–‡æ¡£ {counts['doc']}æ¡ï¼Œé—®ç­”å¯¹ {counts['qa']}æ¡")
                else:
                    counts = trainer.train_all(ddl_list, doc_list, qa_pairs)
                    st.success(f"âœ… æ‰¹é‡è®­ç»ƒå®Œæˆï¼æˆåŠŸï¼šDDL {counts['ddl']}æ¡ï¼Œæ–‡æ¡£ {counts['doc']}æ¡ï¼Œé—®ç­”å¯¹ {counts['qa']}æ¡")
                
                # æ˜¾ç¤ºå‘é‡åº“çŠ¶æ€
                try:
                    db_info = trainer.vector_db.get_collection_info()
                    st.info(f"ğŸ“š å‘é‡åº“å½“å‰æ–‡æ¡£æ€»æ•°: {db_info.get('count', 'N/A')}")
                except:
                    pass
                
            except Exception as e:
                st.error(f"âŒ è®­ç»ƒå¤±è´¥ï¼š{str(e)}")
                st.exception(e)
    
    # æ¸…ç©ºæŒ‰é’®
    if total_items > 0:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºè¾“å…¥"):
            st.rerun()

# ä¾§è¾¹æ ä¿¡æ¯
with st.sidebar:
    st.title("ç³»ç»Ÿä¿¡æ¯")
    st.write("NL2SQL RAG Demo")
    st.write("åŸºäº Qwen + ChromaDB + MySQL")
    
    # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
    st.divider()
    st.subheader("ç³»ç»ŸçŠ¶æ€")
    try:
        db_info = trainer.vector_db.get_collection_info()
        st.metric("å‘é‡åº“æ–‡æ¡£æ•°", db_info.get('count', 0))
    except:
        st.metric("å‘é‡åº“æ–‡æ¡£æ•°", "æœªè¿æ¥")
    
    # æ·»åŠ æµ‹è¯•æ•°æ®æŒ‰é’®
    st.divider()
    if st.button("æ·»åŠ æµ‹è¯•æ•°æ®"):
        test_ddl = [
            "CREATE TABLE btsbase (ID INT PRIMARY KEY, station_name VARCHAR(100), cell_name VARCHAR(100), `çœä»½` VARCHAR(50), `åœ°å¸‚` VARCHAR(50), frequency_band VARCHAR(20))",
            "CREATE TABLE kpibase (ID INT PRIMARY KEY, `å¼€å§‹æ—¶é—´` DATETIME, R1001_012 BIGINT, R1001_001 BIGINT, R1034_012 BIGINT, R1034_001 BIGINT, R1039_002 BIGINT, R1039_001 BIGINT, R2032_012 BIGINT, R2032_001 BIGINT, R1012_001 BIGINT, R1012_002 BIGINT, K1009_001 BIGINT, K1009_002 BIGINT)"
        ]
        test_docs = [
            "btsbaseè¡¨åŒ…å«5GåŸºç«™çš„åŸºç¡€ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŸºç«™åç§°ã€å°åŒºåç§°ã€çœä»½ã€åœ°å¸‚ã€é¢‘æ®µç­‰",
            "kpibaseè¡¨åŒ…å«5Gç½‘ç»œçš„KPIæŒ‡æ ‡æ•°æ®ï¼ŒåŒ…æ‹¬å„ç§æ€§èƒ½è®¡æ•°å™¨çš„å€¼",
            "æ— çº¿æ¥é€šç‡è®¡ç®—å…¬å¼ï¼š100 * (R1001_012/R1001_001) * (R1034_012/R1034_001) * (R1039_002/R1039_001)",
            "5GåŸºç«™æ•°é€šè¿‡COUNT(DISTINCT station_name)ç»Ÿè®¡ï¼Œ5Gå°åŒºæ•°é€šè¿‡COUNT(DISTINCT cell_name)ç»Ÿè®¡"
        ]
        test_qa = [
            {
                "question": "æ¹–åŒ—çœ5G ç½‘ç»œä»·å€¼æŒ‡æ ‡", 
                "sql": "select b.`çœä»½`, k.`å¼€å§‹æ—¶é—´`, b.`frequency_band`, SUM(k.R2032_012) / 1e6 as ä¸‹è¡ŒPDCPå±‚ä¸šåŠ¡æµé‡_GB, SUM(k.R2032_001) / 1e6 as ä¸Šè¡ŒPDCPå±‚ä¸šåŠ¡æµé‡_GB, (SUM(k.R1012_001) + SUM(k.R1012_002)) / 1e6 as æ€»æµé‡_TB, SUM(k.K1009_001) / 4 as VoNRè¯­éŸ³è¯åŠ¡é‡, SUM(k.K1009_002) / 4 as ViNRè§†é¢‘è¯åŠ¡é‡ from btsbase b inner join kpibase k on b.ID = k.ID WHERE b.`çœä»½` = 'æ¹–åŒ—çœ' group by b.`çœä»½`, k.`å¼€å§‹æ—¶é—´`, b.`frequency_band` order by k.`å¼€å§‹æ—¶é—´`;"
            },
            {
                "question": "æ¹–åŒ—çœ5G ç½‘ç»œæ€§èƒ½æŒ‡æ ‡", 
                "sql": "select b.`çœä»½`, k.`å¼€å§‹æ—¶é—´`, b.`frequency_band`, 100 * (SUM(k.R1001_012) / NULLIF(SUM(k.R1001_001), 0)) * (SUM(k.R1034_012) / NULLIF(SUM(k.R1034_001), 0)) * (SUM(k.R1039_002) / NULLIF(SUM(k.R1039_001), 0)) AS æ— çº¿æ¥é€šç‡, 100 * (SUM(k.R1004_003) - SUM(k.R1004_004)) / NULLIF(SUM(k.R1004_002) + SUM(k.R1004_007) + SUM(k.R1005_012) + SUM(k.R1006_012), 0) AS æ— çº¿æ‰çº¿ç‡ FROM btsbase b INNER JOIN kpibase k ON b.ID = k.ID WHERE b.`çœä»½` = 'æ¹–åŒ—çœ' GROUP BY b.`çœä»½`, k.`å¼€å§‹æ—¶é—´`, b.`frequency_band` order by k.`å¼€å§‹æ—¶é—´`;"
            },
            {
                "question": "ç»Ÿè®¡å„åœ°å¸‚çš„5GåŸºç«™æ•°é‡",
                "sql": "SELECT `åœ°å¸‚`, COUNT(DISTINCT station_name) AS `5gåŸºç«™æ•°` FROM btsbase GROUP BY `åœ°å¸‚`;"
            }
        ]
        
        with st.spinner("æ·»åŠ æµ‹è¯•æ•°æ®..."):
            try:
                counts = trainer.train_all(test_ddl, test_docs, test_qa)
                st.success("æµ‹è¯•æ•°æ®æ·»åŠ å®Œæˆï¼")
            except Exception as e:
                st.error(f"æ·»åŠ æµ‹è¯•æ•°æ®å¤±è´¥: {str(e)}")
    
    # æ·»åŠ é€šä¿¡è¡Œä¸šå¸¸è§æŸ¥è¯¢
    if st.button("æŸ¥çœ‹é€šä¿¡è¡Œä¸šç¤ºä¾‹"):
        with st.expander("ç¤ºä¾‹æŸ¥è¯¢", expanded=True):
            st.write("**Q: æŸ¥è¯¢æ¹–åŒ—çœçš„æ— çº¿æ¥é€šç‡è¶‹åŠ¿**")
            st.code("SELECT b.`çœä»½`, k.`å¼€å§‹æ—¶é—´`, 100 * (SUM(k.R1001_012) / NULLIF(SUM(k.R1001_001), 0)) * (SUM(k.R1034_012) / NULLIF(SUM(k.R1034_001), 0)) * (SUM(k.R1039_002) / NULLIF(SUM(k.R1039_001), 0)) AS æ— çº¿æ¥é€šç‡ FROM btsbase b INNER JOIN kpibase k ON b.ID = k.ID WHERE b.`çœä»½` = 'æ¹–åŒ—çœ' GROUP BY b.`çœä»½`, k.`å¼€å§‹æ—¶é—´` ORDER BY k.`å¼€å§‹æ—¶é—´`", language='sql')
            
            st.write("**Q: ç»Ÿè®¡å„åœ°å¸‚çš„5GåŸºç«™æ•°é‡**")
            st.code("SELECT `åœ°å¸‚`, COUNT(DISTINCT station_name) AS `5gåŸºç«™æ•°` FROM btsbase GROUP BY `åœ°å¸‚`;", language='sql')
            
            st.write("**Q: æŸ¥è¯¢5Gç½‘ç»œæµé‡æŒ‡æ ‡**")
            st.code("SELECT b.`çœä»½`, SUM(k.R2032_012) / 1e6 as ä¸‹è¡Œæµé‡_GB, SUM(k.R2032_001) / 1e6 as ä¸Šè¡Œæµé‡_GB FROM btsbase b INNER JOIN kpibase k ON b.ID = k.ID GROUP BY b.`çœä»½`", language='sql')




